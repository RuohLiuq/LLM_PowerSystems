{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes.vectorstore import VectorstoreIndexCreator, VectorStoreIndexWrapper\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "import openai\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "# openai.api_key = \"\"\n",
    "openai.api_key = \"\"\n",
    "local_persist_path = './vector_store'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_file = '/root/LLM/LangChain/E-1-RM22-12-000.pdf'\n",
    "assist_file_head = 'FERC_assist'\n",
    "file_head = 'FERC'\n",
    "\n",
    "# 1 none 2 PE 3 PE/assist\n",
    "\n",
    "assist = False\n",
    "use_PE = True\n",
    "fewshot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_path(index_name):\n",
    "    return os.path.join(local_persist_path, index_name)\n",
    "\n",
    "# save separately\n",
    "def load_pdf_and_save_to_index(file_path, index_name):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    index = VectorstoreIndexCreator(vectorstore_kwargs={'persist_directory': get_index_path(index_name)}).from_loaders({loader})\n",
    "    index.vectorstore.persist()\n",
    "\n",
    "# save together\n",
    "def load_multiple_pdf_and_save_to_index(file_paths, index_name):\n",
    "    loaders = [PyPDFLoader(file_path) for file_path in file_paths]\n",
    "    print(loaders)\n",
    "    index = VectorstoreIndexCreator(vectorstore_kwargs={'persist_directory': get_index_path(index_name)}).from_loaders(loaders)\n",
    "    index.vectorstore.persist()\n",
    "\n",
    "def load_index(index_name):\n",
    "    index_path = get_index_path(index_name)\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    vectordb = Chroma(persist_directory=index_path, embedding_function=embedding)\n",
    "    return VectorStoreIndexWrapper(vectorstore=vectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/root/LLM/LangChain/assist_files/'\n",
    "names = os.listdir(root)\n",
    "# assist_files = [os.path.join(root, name) for name in names]\n",
    "assist_files = [\n",
    "    '/root/LLM/LangChain/assist_files/Chapter-10---Distribution-System_2017_Electrical-Power-Systems.pdf',\n",
    "    '/root/LLM/LangChain/assist_files/Chapter-24---Renewable-Energy-Sources_2017_Electrical-Power-Systems.pdf',\n",
    "    '/root/LLM/LangChain/assist_files/Chapter-20---Economic-Operation-of-Power-System_2017_Electrical-Power-System.pdf'\n",
    "]\n",
    "\n",
    "# load_pdf_and_save_to_index(main_file, 'main_file')\n",
    "\n",
    "# for i in range(len(assist_files)):\n",
    "#     if not os.path.exists(root + '{}_{}'.format(assist_file_head,names[i])):\n",
    "#         load_pdf_and_save_to_index(assist_files[i], '{}_{}'.format(assist_file_head,names[i]))\n",
    "#     time.sleep(20)\n",
    "\n",
    "if assist:\n",
    "    file_paths = [main_file] + assist_files\n",
    "    load_multiple_pdf_and_save_to_index(file_paths, assist_file_head)\n",
    "else:\n",
    "    file_path = main_file\n",
    "    load_pdf_and_save_to_index(file_path, 'FERC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assistance (only support for 1-3 files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like too many files will degrade the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assist_index_list = []\n",
    "\n",
    "# if assist == True:\n",
    "#     for n in os.listdir('vector_store/'):\n",
    "#         if n.startswith(assist_file_head):\n",
    "#             assist_index_list.append(n)\n",
    "\n",
    "# if len(assist_index_list) == 0:\n",
    "#     index_list = ['main_file']\n",
    "# else:\n",
    "#     index_list = ['main_file'] + assist_index_list\n",
    "\n",
    "# index = load_index(index_list)\n",
    "\n",
    "if assist:\n",
    "    index = load_index('FERC_assist')\n",
    "else:\n",
    "    index = load_index('FERC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = 'experienced'\n",
    "role = 'policy maker'\n",
    "field = 'power system'\n",
    "skill = 'summarize long text'\n",
    "character_prompt = 'You are a {} {} in {}, who is good at {}. '.format(style, role, field, skill)\n",
    "\n",
    "augmentation_prompt = 'Let us think step by step. '\n",
    "\n",
    "question_prompt = 'Now, pay attention! My question is: '\n",
    "\n",
    "include = 'as many technical details as possible. '\n",
    "include_prompt = 'In your answer, you should include {}'.format(include)\n",
    "\n",
    "# format_prompt = 'Your answer should be like this format: Answer: ...'\n",
    "format_prompt = ''\n",
    "\n",
    "request_prompt = 'Please do not copy! If applicable, you should not include too much original content in the file, unless the original content is strongly needed.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "few-shot (not available now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_prompt = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_question_list = [\n",
    "        'Please summarize this file in 200 words.',\n",
    "        'What is the structure of this file?',\n",
    "        'Please give me several key words strongly related to this file.',\n",
    "        'Please give me several key words strongly related to the content in this file. Note that, do not select words without meaning.',\n",
    "]\n",
    "\n",
    "technical_question_list = [\n",
    "        'What is Synchronization?',\n",
    "        'What is the Synchronization in this file?',\n",
    "        'What is the Synchronization in this file? Please explain it in 200 words.',\n",
    "        'Please summarize the main content of Synchronization (200) part in this file.',\n",
    "        'Please give an example in real-world application of the synchronization mentioned in this file.',\n",
    "        'Please tell me, in Reliability Standards in this file, which standard is the most useful one?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "i = 1\n",
    "for q in general_question_list:\n",
    "    if use_PE and fewshot:\n",
    "\n",
    "        if assist:\n",
    "            file_name = \"output_files/{}_PE_FS_AS_general_output.json\".format(assist_file_head)\n",
    "        else:\n",
    "            file_name = \"output_files/{}_PE_FS_general_output.json\".format(assist_file_head)\n",
    "    elif use_PE:\n",
    "        prompt = character_prompt + augmentation_prompt + question_prompt + q + include_prompt + format_prompt + request_prompt\n",
    "\n",
    "        if assist:\n",
    "            file_name = \"output_files/{}_PE_AS_general_output.json\".format(assist_file_head)\n",
    "        else:\n",
    "            file_name = \"output_files/{}_PE_general_output.json\".format(assist_file_head)\n",
    "    else:\n",
    "        prompt = q\n",
    "\n",
    "        if assist:\n",
    "            file_name = \"output_files/{}_AS_general_output.json\".format(assist_file_head)\n",
    "        else:\n",
    "            file_name = \"output_files/{}_general_output.json\".format(assist_file_head)\n",
    "\n",
    "    result = index.query_with_sources(prompt, chain_type='map_reduce')\n",
    "    data.update({\"question{}\".format(i): result['question'], \"answer{}\".format(i): result['answer']})\n",
    "    time.sleep(60)\n",
    "    i += 1\n",
    "\n",
    "with open(file_name, \"w\") as outfile:\n",
    "    json.dump(data, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "i = 1\n",
    "for q in technical_question_list:\n",
    "    if use_PE and fewshot:\n",
    "        prompt = character_prompt + augmentation_prompt + fs_prompt + question_prompt + q + include_prompt + format_prompt + request_prompt\n",
    "\n",
    "        if assist:\n",
    "            file_name = \"output_files/{}_PE_FS_AS_technical_output.json\".format(assist_file_head)\n",
    "        else:\n",
    "            file_name = \"output_files/{}_PE_FS_technical_output.json\".format(assist_file_head)\n",
    "    elif use_PE:\n",
    "        prompt = character_prompt + augmentation_prompt + question_prompt + q + include_prompt + format_prompt + request_prompt\n",
    "\n",
    "        if assist:\n",
    "            file_name = \"output_files/{}_PE_AS_technical_output.json\".format(assist_file_head)\n",
    "        else:\n",
    "            file_name = \"output_files/{}_PE_technical_output.json\".format(assist_file_head)\n",
    "    else:\n",
    "        prompt = q\n",
    "\n",
    "        if assist:\n",
    "            file_name = \"output_files/{}_AS_technical_output.json\".format(assist_file_head)\n",
    "        else:\n",
    "            file_name = \"output_files/{}_technical_output.json\".format(assist_file_head)\n",
    "\n",
    "    result = index.query_with_sources(prompt, chain_type='map_reduce')\n",
    "    data.update({\"question{}\".format(i): result['question'], \"answer{}\".format(i): result['answer']})\n",
    "    time.sleep(60)\n",
    "    i += 1\n",
    "\n",
    "with open(file_name, \"w\") as outfile:\n",
    "    json.dump(data, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation chain\n",
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "loader = PyPDFLoader(\"/root/LLM/LangChain/E-1-RM22-12-000.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "#\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap = 100,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "#\n",
    "pages_split = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "def transform_func(inputs: dict) -> dict:\n",
    "    text = inputs[\"text\"]\n",
    "    shortened_text = \"\\n\\n\".join(text.split(\"\\n\\n\")[:3])\n",
    "    return {\"output_text\": shortened_text}\n",
    "\n",
    "# 1\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"text\"], output_variables=[\"output_text\"], transform=transform_func\n",
    ")\n",
    "\n",
    "paragraph = ''\n",
    "for i in range(20):\n",
    "    paragraph += pages_split[i].page_content\n",
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "i = 0\n",
    "for q in general_question_list:\n",
    "    # 2\n",
    "    if use_PE:\n",
    "        template = character_prompt + augmentation_prompt + fs_prompt + question_prompt + q + \"{output_text}.\" + include_prompt + format_prompt + request_prompt\n",
    "    else:\n",
    "        template = q + \"{output_text}.\"\n",
    "    prompt = PromptTemplate(input_variables=[\"output_text\"], template=template)\n",
    "    llm_chain = LLMChain(llm=OpenAI(), prompt=prompt)\n",
    "\n",
    "    # 3\n",
    "    sequential_chain = SimpleSequentialChain(chains=[transform_chain, llm_chain])\n",
    "    result = sequential_chain.run(paragraph)\n",
    "    data.update({\"question{}\".format(i): template, \"answer{}\".format(i): result})\n",
    "    i += 1\n",
    "    time.sleep(60)\n",
    "\n",
    "file_name = \"output_files/{}_general_output.json\".format(assist_file_head)\n",
    "\n",
    "with open(file_name, \"w\") as outfile:\n",
    "    json.dump(data, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "i = 0\n",
    "for q in technical_question_list:\n",
    "    # 2\n",
    "    if use_PE:\n",
    "        template = character_prompt + augmentation_prompt + fs_prompt + question_prompt + q + \"{output_text}.\" + include_prompt + format_prompt + request_prompt\n",
    "    else:\n",
    "        template = q + \"{output_text}.\"\n",
    "    prompt = PromptTemplate(input_variables=[\"output_text\"], template=template)\n",
    "    llm_chain = LLMChain(llm=OpenAI(), prompt=prompt)\n",
    "\n",
    "    # 3\n",
    "    sequential_chain = SimpleSequentialChain(chains=[transform_chain, llm_chain])\n",
    "    result = sequential_chain.run(paragraph)\n",
    "    data.update({\"question{}\".format(i): template, \"answer{}\".format(i): result})\n",
    "    i += 1\n",
    "    time.sleep(60)\n",
    "\n",
    "file_name = \"output_files/{}_technical_output.json\".format(assist_file_head)\n",
    "\n",
    "with open(file_name, \"w\") as outfile:\n",
    "    json.dump(data, outfile, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
